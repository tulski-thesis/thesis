\newpage


\section{Projekt i wykonanie scrapera}\label{sec:projekt-scrapera}

Niniejszy rozdział poświęcono projektowi i wykonaniu scrapera - narzędzia służącego do automatycznego zbierania danych ze sklepu internetowego tulski, omówionego w \autoref{sec:projekt-platformy}.
Założono, że celem scrapera jest zgromadzenie informacji o wszystkich produktach dostępnych w sklepie internetowym, które mogą zostać wykorzystane do analizy asortymentu i cen produktów.
Kolejnym istotnym założeniem rozdziału jest podejście typu czarnej skrzynki (ang. \emph{black-box})\cite{sekurak-testy-penetracyjne}.
Brak początkowej wiedzy na temat struktury i mechanizmów scrapowanego serwisu, jakim jest sklep internetowy tulski, nadaje realizm opisanym działaniom, odzwierciedlając warunki, w jakich najczęściej pracują osoby specjalizujące się w web scrapingu.
Poniższa część pracy jest zatem nie tylko technicznym studium przypadku, ale również wglądem w procesy myślowe i metody, które są wykorzystywane w realnych warunkach.

\subsection{Rekonesans}\label{subsec:rekonesans}

Głównym zadaniem rekonesansu było zidentyfikowanie metod dających efektywny dostęp do interesujących danych.
Skupiono się na zrozumieniu mechanizmu ładowania i renderowania treści, strukturze zasobów oraz wykryciu potencjalnych wyzwań, które mogłyby wpłynąć na proces scrapowania.
Rekonesans jest kluczowym etapem w tworzeniu scrapera, ponieważ od niego zależą efektywność, awaryjność i stabilność narzędzia.
Przykładowo, jeśli podczas rekonesansu uda się znaleźć żądania API, które zwracają dane w formacie JSON, narzędzie będzie potencjalnie szybsze i bardziej odporne na zmiany.
Wynika to z faktu, że kontrakt API zmienia się z reguły rzadziej niż interfejs graniczny użytkownika.

Znamiennym elementem rekonesansu była analiza ruchu sieciowego za pomocą narzędzi programistycznych wbudowanych w przeglądarkę internetową Firefox.
Po pierwsze, ustalono, że strona korzysta z proxy Cloudflare, co wskazuje nagłówek \emph{server} (zob. \autoref{lst:rekonesans-get-homepage}).
Po drugie, nagłówek \emph{x-powered-by} manifestuje użycie frameworka Next (zob. \autoref{lst:rekonesans-get-homepage}).
Wykorzystanie Next implikuje wykorzystanie biblioteki React, co sugeruje dynamiczne renderowanie treści.

\begin{listing}[H]
    \begin{minted}[xleftmargin=10pt,linenos,breaklines]{bash}
$ curl -I https://store.tulski.com
HTTP/2 200
date: Sun, 17 Dec 2023 17:09:05 GMT
content-type: text/html; charset=utf-8
cache-control: private, no-cache, no-store, max-age=0, must-revalidate
vary: RSC, Next-Router-State-Tree, Next-Router-Prefetch, Next-Url, Accept-Encoding
x-powered-by: Next.js
cf-cache-status: DYNAMIC
report-to: {"endpoints":[{"url":"https:\/\/a.nel.cloudflare.com\/report\/v3 ?s=l8mwfqVrt2h%2BXJYVshGNiHz%2FIRwSCEqIgVqxZ8kBhWWqkIGQaAuYiOy9 sv4JmNEGP1%2B53j1pVTVnm%2BpX2sID4%2BRDooubYk4tibHwMUz8sjuDxll1N rjXEffI6gReu1VJV7JI"}], "group":"cf-nel", "max_age":604800}
nel: {"success_fraction":0,"report_to":"cf-nel","max_age":604800}
server: cloudflare
cf-ray: 8370c5927feb666e-AMS
alt-svc: h3=":443"; ma=86400
    \end{minted}
    \caption{Nagłówki odpowiedzi dla strony domowej sklepu tulski}
    \label{lst:rekonesans-get-homepage}
\end{listing}

Po trzecie, w trakcie rekonesansu zauważono żądania do serwera API \emph{api.tulski.com}.
Odkrycie potwierdza hipotezę o dynamicznym renderowaniu treści na stronie.
Serwer API, podobnie jak witryna internetowa, korzysta z proxy Cloudflare.
Zidentyfikowano dwa żądania, które zwracają dane o produktach w formacie JSON\@.
Jednym z zidentyfikowanych żądań jest to, które zwraca listę produktów (zob. \autoref{fig:store-get-products-list}), wykorzystując paginację.
W żądaniu rozmiar strony jest określany parametrem \emph{limit}, a pozycję startową strony - parametrem \emph{offset}.

Przeprowadzono testy metodą prób i błędów, aby określić maksymalną liczbę produktów, jaką może zwrócić API w jednej odpowiedzi.
Ustalono, że parametr \emph{limit} nie posiada ścisłej walidacji, co umożliwia pobranie znacznie większej ilości danych niż to, co zwykle pobiera klient aplikacji internetowej.
Zauważono natomiast, że ograniczeniem podczas pobierania dużej liczby rekordów jest czas otwarcia połączenia.
W przypadkach, gdy próbowano pobrać bardzo dużą liczbę rekordów, na przykład 8500 (zob.~\autoref{lst:store-get-products-list-8500}), serwer odpowiedział statusem \emph{504 Gateway Timeout}, wskazując na przekroczenie maksymalnego dopuszczalnego czasu przetwarzania żadania.
Maksymalną liczbą rekordów jaką udało się pobrać bez błędów było 8000 (zob.~\autoref{lst:store-get-products-list-8000}).

\begin{figure}[p]
    \centering
    \includegraphics[width=0.8\textwidth]{img/store-get-products-list}
    \caption{Żądanie zwracające listę produktów}
    \label{fig:store-get-products-list}
\end{figure}
\begin{listing}[p]
    \begin{minted}[xleftmargin=10pt,linenos]{bash}
$ curl -X GET 'https://api.tulski.com/store/products?limit=8500' \
        -s \
        -o /dev/null \
        -w 'HTTP Code: %{http_code}\nTime Total: %{time_total}s\n'
HTTP Code: 504
Time Total: 61.691521s
    \end{minted}
    \caption{Żądanie 8500 produktów}
    \label{lst:store-get-products-list-8500}
\end{listing}
\begin{listing}[p]
    \begin{minted}[xleftmargin=10pt,linenos]{bash}
$ curl -X GET 'https://api.tulski.com/store/products?limit=8000' \
        -s \
        -o /dev/null \
        -w 'HTTP Code: %{http_code}\nTime Total: %{time_total}s\n'
HTTP Code: 200
Time Total: 60.223100s
    \end{minted}
    \caption{Żądanie 8000 produktów}
    \label{lst:store-get-products-list-8000}
\end{listing}

\newpage

Następne żądanie API, które zwróciło uwagę podczas rekonesansu, dotyczyło pobierania szczegółowych danych o produkcie (zob. \autoref{fig:store-get-product-details}).
Odpowiedź serwera na to żądanie zawiera bogaty zestaw danych o produkcie, w tym nazwę, opis, zdjęcia, linki do obrazów, metadane oraz szczegóły dotyczące rozmiarów i wariantów z ich dostępności.
Struktura danych w formacie JSON, wykorzystana w odpowiedzi (zob. Listing ~\autoref{lst:store-get-product-details-request}), jest przejrzysta i dobrze zorganizowana.
Zwracane dane wyczerpują wymagania postawione przed scraperem.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{img/store-get-product-details}
    \caption{Żądanie po szczegóły produktu}
    \label{fig:store-get-product-details}
\end{figure}
\begin{listing}[H]
    \begin{minted}[xleftmargin=10pt,linenos,breaklines]{bash}
$ curl -G -d "id=prod_01HDFP2PJHJHZ6N1S2W0E3RXP9" \
        "https://api.tulski.com/store/products"
{"products": [{"id": "prod_01HDFP2PJHJHZ6N1S2W0E3RXP9", "title": "Rocia Women Gold Toned & Beige Heels", "subtitle": null, "status": "published", "external_id": null, "description": "Style Note\n\n\nCreate\n\n", "handle": "rocia-women-gold-toned-and-beige-heels-50168", "is_giftcard": false, "discountable": true, "thumbnail": ...
    \end{minted}
    \caption{Struktura JSON odpowiedzi na żądanie po szczegóły produktu}
    \label{lst:store-get-product-details-request}
\end{listing}

\subsection{Implementacja}\label{subsec:scraper-implementation}

Po uzyskaniu odpowiedniej ilości informacji rozpoczęto proces tworzenia scrapera.
Podczas rekonesansu ustalono, że strona jest renderowana dynamicznie i pobiera dane, w formacie JSON, z końcówek API\@.
Stąd też zdecydowano się na API scraping.
Wykorzystano język programowania TypeScript, środowisko uruchomieniowe Node.js i bibliotekę got.

\subsubsection{Konfiguracja klienta HTTP}\label{subsubsec:storeclient}

Skonfigurowano klienta HTTP \texttt{storeClient}, który wysyła żądania do serwera API z nagłówkami typowymi dla przeglądarki internetowej.
Działanie to miało na celu jak najwierniejsze odwzorowanie śladu i zachowań scrapera na wzór klienta przeglądarkowego, co zwiększa jego dyskrecję podczas działania.
Ponadto dodano logowanie żądań i odpowiedzi, co znacząco ułatwia proces debugowania, monitorowania i identyfikacji potencjalnych błędów.
Konfiguracja została przedstawiona na \autoref{lst:store-http-client-config}.

\begin{listing}[H]
    \begin{minted}[xleftmargin=10pt,linenos,breaklines]{typescript}
const storeClient: typeof got = got.extend({
  prefixUrl: "https://api.tulski.com/",
  hooks: {
    beforeRequest: [requestLogger(logger)],
    afterResponse: [responseLogger(logger)],
  },
  headers: {
    "User-Agent":
      "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:121.0) Gecko/20100101 Firefox/121.0",
    Accept: "*/*",
    "Accept-Language": "en-US,en;q=0.7,pl;q=0.3",
    "Accept-Encoding": "gzip, deflate, br",
    "Access-Control-Request-Method": "GET",
    DNT: "1",
    "Sec-GPC": "1",
    Connection: "keep-alive",
    "Sec-Fetch-Dest": "empty",
    "Sec-Fetch-Mode": "cors",
    "Sec-Fetch-Site": "same-site",
    Pragma: "no-cache",
    "Cache-Control": "no-cache",
  },
});
    \end{minted}
    \caption{Konfiguracja klienta HTTP storeClient}
    \label{lst:store-http-client-config}
\end{listing}

\subsubsection{Funkcja pobierająca listę produktów}

Następnie, zaimplementowano funkcje \texttt{getProductsList} służącą do pobierania listy produktów (zob. \autoref{lst:store-http-client-getProductsList}).
Wykorzystano Pagination API biblioteki got\cite{got-paginate-docs}, które pozwala na iteracyjne pobieranie kolejnych pozycji z listy produktów.
Lista 500 produktów jest zwracana w żądaniu \texttt{GET /store/products}.
Funkcja \texttt{transform} parsuje otrzymaną odpowiedź (\texttt{Response}) do listy produktów.
Funkcja \texttt{paginate} zwraca atrybuty wskazujące na następną stronę. Wylicza nową wartość parametru offset wykorzystując aktualną długość listy produktów i wartość offset.

\begin{listing}[H]
    \begin{minted}[xleftmargin=10pt,linenos,breaklines]{typescript}
export const getProductsList = () => {
  return storeClient.paginate({
    url: "store/products",
    searchParams: {
      currency_code: "eur",
      is_giftcard: false,
      limit: 500
    },
    pagination: {
      transform: (response: Response<string>): ProductListing[] => {
        return JSON.parse(response.body).products;
      },
      paginate: ({ response, currentItems }) => {
        if (currentItems.length === 0) {
          return false;
        }
        const searchParams = response.request.options
          .searchParams as URLSearchParams;
        const previousOffset = Number(searchParams?.get("offset") ?? 0);
        return {
          searchParams: {
            offset: previousOffset + currentItems.length,
          },
        };
      },
      countLimit: 5000,
      // Wait before making another request to prevent rate limiting
      backoff: 500,
      requestLimit: 1000,
      stackAllItems: false,
    },
  });
};
    \end{minted}
    \caption{Funkcja pobierająca listę produktów}
    \label{lst:store-http-client-getProductsList}
\end{listing}

\subsubsection{Funkcja pobierająca szczegóły produktu}

Ostatnią zaimplementowaną funkcją wykorzystującą \texttt{storeClient} jest funkcja \texttt{get-\\ProductDetails} (zob. \autoref{lst:store-http-client-getProductDetails}).
Pobiera ona szczegółowe dane o produkcie z wskazanym w argumencie identyfikatorze.
Wykorzystuje żądanie \texttt{GET /store/products/\\{{productId}}} odkryte podczas rekonesansu.

\begin{listing}[H]
    \begin{minted}[xleftmargin=10pt,linenos,breaklines]{typescript}
export const getProductDetails = (
  productId: string,
): CancelableRequest<ProductResponse> => {
  return storeClient
    .get(`store/products/${productId}`, {
      searchParams: {
        currency_code: "eur",
      },
    })
    .json();
};
    \end{minted}
    \caption{Funkcja pobierająca szczegóły produktu}
    \label{lst:store-http-client-getProductDetails}
\end{listing}

\subsubsection{Mechanizm kolejkujący zadania}

Scraper wyposażono w mechanizm kolejkujący zadania.
Jest to powszechnie znane i używane rozwiązanie w tego rodzaju oprogramowaniu.
Należy pamiętać, że domeną scraperów jest bardzo duża liczba asynchronicznych procesów (m.in.~zapytania HTTP).
Wybranymi zaletami mechanizmów kolejkujących w scraperach są:
\begin{itemize}
    \item efektywne zarządzanie zasobami (np. karty sieciowej),
    \item kontrola nad liczbą i częstotliwością zapytań do serwera docelowego, co jest istotne dla uniknięcia blokowania,
    \item ułatwiona monitoring i obsługa błędów poprzez możliwość ponownego przetwarzania zadania,
    \item skalowalność i elastyczność scrapera.
\end{itemize}
\noindent \autoref{lst:queue-implementation} przedstawia implementację klasy \texttt{Queue} reprezentującej asynchroniczną kolejkę zadań.
Wykorzystano zewnętrzną bibliotekę queue, która została dodatkowo przykryta własną warstwą abstrakcji, tak aby ułatwić jej dalsze wykorzystanie.

\begin{listing}[p]
    \begin{minted}[xleftmargin=10pt,linenos,breaklines]{typescript}
import { default as ExternalQueue } from "queue";

export interface QueueOptions {
  concurrency: number;
}

export class Queue<T> {
  private readonly queue: ExternalQueue;

  constructor(options: QueueOptions) {
    this.queue = new ExternalQueue({
      concurrency: options.concurrency,
      autostart: true,
      results: [],
    });
  }

  get results() {
    return (this.queue.results || []).filter((el) => el !== null);
  }

  get finishedJobsCount() {
    return this.results.length;
  }

  get jobsCount() {
    return this.queue.length;
  }

  set onSuccess(callback: () => void) {
    this.queue.addEventListener("success", callback);
  }

  push(job: () => Promise<T>) {
    this.queue.push(job);
  }

  async waitUntilEnd() {
    await new Promise<void>((resolve, reject) => {
      this.queue.addEventListener("end", () => {
        resolve();
      });
    });
  }
}

    \end{minted}
    \caption{Implementacja klasy Queue}
    \label{lst:queue-implementation}
\end{listing}

\subsubsection{Zapis danych}

Pobrane dane należy zapisać.
W tym celu zaimplementowano klasę \texttt{LocalDirectory-\\Bucket} (zob. \autoref{lst:storage-save-product}).
Jej metoda save zapisuje otrzymane dane w postaci tekstowej pod wskazaną ścieżką.
Pliki są zapisywane w jednym katalogu w lokalnym systemie plików.

Użycie powszechnie znanej koncepcji \texttt{Bucket} jako interfejsu pozostawia oprogramowanie otwarte na zmiany.
Scraper może wykorzystywać dowolny system plików, na przykład Amazon S3 lub Google Cloud Storage.

\begin{listing}[p]
    \begin{minted}[xleftmargin=10pt,linenos,breaklines]{typescript}
import { writeFile, mkdir, access } from "fs/promises";
import path from "node:path";

export interface LocalDirectorBucketConfig {
  directory: string;
}

export class LocalDirectoryBucket implements Bucket {
  private directoryExists: undefined | true;

  constructor(private readonly config: LocalDirectorBucketConfig) {}

  async save(filepath: string, data: string | Record<any, any>) {
    await this.ensureDirExists();
    const stringifiedData =
      typeof data === "string" ? data : JSON.stringify(data);
    const fullPath = path.join(this.config.directory, filepath);
    return writeFile(fullPath, stringifiedData);
  }

  private async ensureDirExists() {
    if (this.directoryExists) {
      return;
    }
    try {
      await access(this.config.directory);
    } catch (error: any) {
      if (error.code === "ENOENT") {
        await mkdir(this.config.directory, { recursive: true });
      }
    }
    this.directoryExists = true;
  }
}
    \end{minted}
    \caption{Klasa LocalDirectoryBucket}
    \label{lst:storage-save-product}
\end{listing}

\newpage

\subsubsection{Punkt wejścia programu}

Punktem wejścia programu jest samowywołująca się funkcja asynchroniczna (ang. \emph{Self-Invoking Function}) zarządzająca procesem scrapowania (zob. \autoref{lst:scraper-main}).
Opisywana funkcja kolejno:
\begin{enumerate}
    \item Tworzy zmienną \texttt{timestamp}, która przechowuje znacznik czasowy (ang. \emph{timestamp}) identyfikujący bieżące uruchomienie programu.
    \item Tworzy zmienną \texttt{outDir}, która będzie ścieżką katalogu wyjściowego;
    składową \texttt{outDir} jest wcześniej stworzony identyfikator.
    \item Inicjuje zmienną \texttt{bucket} jako instancję klasy \texttt{LocalDirectoryBucket} z ścieżką \texttt{outDir}.
    \item Inicjuje zmienną \texttt{queue} jako instancję klasy \texttt{Queue} z ograniczeniem równolegle wykonywanych zadań do 100.
    \item Definiuje akcję wykonywaną po każdym skończonym zadaniu w kolejce, która loguje jej aktualny stan;
    \item Dla każdego produktu z listy produktów (\texttt{getProductsList}) dodaje do kolejki zadanie, które
    pobiera szczegóły produktu (\texttt{getProductDetails}) i zapisuje je do pliku JSON\@.
\end{enumerate}

\begin{listing}[H]
    \begin{minted}[xleftmargin=10pt,linenos,breaklines]{typescript}
(async () => {
  const timestamp = new Date();
  const outDir = path.join(
    "out",
    timestamp.toISOString().replace(/\D/g, "_").slice(0, -1),
  );
  const bucket = new LocalDirectoryBucket({ directory: outDir });
  const queue = new Queue({
    concurrency: 100,
  });
  queue.onSuccess = () => {
    logger.info(
      "Jobs finished: %d, left in queue: %d",
      queue.finishedJobsCount,
      queue.jobsCount,
    );
  };
  for await (const productListing of getProductsList()) {
    queue.push(async () => {
      const { product } = await getProductDetails(productListing.id);
      await bucket.save(product.id + ".json", product);
    });
  }
  await queue.waitUntilEnd();
})();
    \end{minted}
    \caption{Punkt wejścia programu}
    \label{lst:scraper-main}
\end{listing}

