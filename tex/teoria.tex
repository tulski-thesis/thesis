\newpage


\section{Web Scraping}\label{sec:teoria}

Web Scraping, znany również jako Web Data Extraction, Web Data Scraping, Web Harvesting i Screen scraping, to technika pozyskiwania informacji z zasobów WWW~\cite{Zhao2017}.
Proces ten najczęściej wykorzystuje automatyzację za pomocą dedykowanego oprogramowania, choć niektóre źródła dopuszczają stosowanie metod manualnych\cite{applications-and-tools}
Zautomatyzowanie web scrapingu, z wykorzystaniem takich narzędzi, umożliwia efektywne i szybkie pozyskiwanie ogromnych ilości danych, liczących setki tysięcy, miliony, a nawet miliardy rekordów.

\subsection{Proces Web Scrapingu}\label{subsec:web-scraping-process}

Proces Web Scrapingu, jak przedstawiono na \autoref{fig:scraping-process}, można podzielić na trzy główne etapy: pobieranie danych, konwersja i przetwarzanie oraz zapis i prezentacja informacji\cite{persson}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{img/scraping-process}
    \caption{Proces Web Scrapingu}
    \label{fig:scraping-process}
\end{figure}

\subsubsection{Pobieranie danych}

Pierwszy etapem procesu jest pobranie danych zawierających interesujące nas treści.
Zazwyczaj realizuje się to poprzez wysłanie zapytań HTTP (ang. \emph{Hypertext Transfer Protocol}) do jednego lub wielu serwerów WWW\@.
Pobrane, surowe dane zwykle są w formacie HTML (ang. \emph{Hypertext Markup Language}), XML (ang. \emph{Extensible Markup Language}) lub JSON (ang. \emph{JavaScript Object Notation}).

To kluczowy etap z perspektywy cyberbezpieczeństwa, ponieważ to właśnie w nim scraper wchodzi w bezpośrednią interakcję ze scrapowaną infrastrukturą.

W związku z zabezpieczeniami stosowanymi przez serwery, ich dostępnością, wydajnością oraz ograniczeniami sieciowymi etap ten jest zwykle najdłuższym w całym procesie.

\subsubsection{Parsowanie, ekstrakcja i przetwarzanie danych}

Drugi etap procesu obejmuje operacje przekształcenia wcześniej pobranych danych, takie jak:
\begin{enumerate}
    \item parsowanie - przekształcenie danych w strukturalną reprezentację łatwiejszą do dalszego przetwarzania;
    \item pkstrakcja - wyodrębnienie interesujących treści;
    \item piltracja - usunięcie niepożądanych i błędnych danych;
    \item mapowanie - przekształcenie danych do pożądanego, jednolitego formatu.
\end{enumerate}

\noindent W przypadku scrapowania danych z wielu różnych źródeł kluczowe jest ich dopasowanie do jednego wspólnego interfejsu, co ułatwia połączenie ich w spójny zbiór danych.

\subsubsection{Zapis i prezentacja informacji}

Ostatni etap Web Scrapingu, obejmujący zapis i prezentację informacji, odgrywa kluczową rolę w udostępnianiu zebranych danych użytkownikom końcowym w przystępnej i zrozumiałej formie\cite{iee-state-of-the-art}.

Dane mogą zostać zapisane w różny sposób, w zależności od potrzeb i preferencji użytkowników docelowych.
Najczęściej stosuje się bazy danych, pliki tekstowe lub arkusze kalkulacyjne, takie jak CSV (ang. \emph{Comma-Separated Values}) czy XLSX (ang. \emph{Microsoft Excel Open XML Spreadsheet}).
Kluczowe jest, aby dane były zapisywane w formatach umożliwiających łatwe przetwarzanie i analizę, aby ułatwić ich dalsze wykorzystanie\cite{iee-state-of-the-art}.

Prezentacja danych to proces konwersji zebranych informacji do formy tekstowej lub wizualnej, dostosowanej do łatwego zrozumienia i interpretacji przez użytkowników.
Ten krok może obejmować tworzenie wykresów, tabel, raportów, interfejsów użytkownika oraz innych wizualizacji, które umożliwiają szybką analizę i wnioskowanie na podstawie dostępnych danych.
Celem jest przedstawienie informacji w klarowny, przystępny sposób dostarczający odpowiedzi na pytania odbiorców.

\subsection{Scraper}\label{subsec:scraper}

Web Scraper, w uproszczeniu scraper, to specjalny rodzaj bota przeprowadzający zautomatyzowany proces web scrapingu.

Jak wspomniano w \autoref{subsec:web-scraping-process} pobieranie danych, czyli etap bezpośredniej interakcji scrapera ze scrapowaną infrastrukturą,
jest jedynym momentem w którym możliwa jest jego identyfikacja i zablokowanie.
Wybrane cechy ruchu sieciowego wykonywanego przez tego typu boty kluczowe w ich detekcji to:

\begin{itemize}
    \item nagłówek \mintinline{text}{User-Agent} wskazujący na narzędzie programistyczne --- wget, curl, Postman, axios, got itd,
%    \item declares its user-agent as being wget, curl, webcopier etc - it's probably a bot.
    \item nie posiadają nagłówka \mintinline{text}{User-Agent},
%    \item no user-agent (or matching a pattern of known bad ones) - it's probably a bot.
    \item nie posiadają ciasteczek ignorując je,
%    \item no cookie, and wont honor a set cookie - it's probably a bot.
    \item rekurencyjne zapytania o coraz bardziej szczegółowe dane,
%    \item requests details -> details -> details -> details ad nauseum - it's probably a bot.
    \item zapytania jedynie zawartość HTML z ominięciem CSS lub JS,
%    \item requests the html, but not .css, .js or site furniture - it's probably a bot.
    \item duża liczba zapytań HTTP z kodami odpowiedzi > 400,
%    \item generates a large number of HTTP error codes > 400 (1.e 401, 403, 404 \& 500)- it's probably a bot.
    \item pochodzą z mało prawdopodobnego źródła ruchu dla ludzi (np\. Amazon AWS),
%    \item originates from an unlikely human traffic source (i.e Amazon AWS) - it's probably a bot.
    \item nigdy nie posiadają nagłówka z adresem odsyłającym - \mintinline{text}{Referer},
%    \item no referrer, ever - it's probably a bot.
    \item sesje z dużą liczbą zapytań\cite{bot-buster}.
%    \item sessions with a lot of hits. it's probably a bot.
\end{itemize}

\subsection{Wybrane metody Web Scrapingu}\label{subsec:web-scraping-methods}

\subsubsection{Kopiuj-Wklej}

Pierwsza i najbardziej podstawowa metoda web scrapingu to kopiuj-wklej.
Jest to proces manualny polegający na wybieraniu i kopiowaniu danych bezpośrednio z witryn internetowych, a następnie wklejaniu ich do pliku lub bazy danych.
Metoda ta jest czasochłonna i nieefektywna dla dużych ilości danych, ale może być skuteczna w przypadku małych i nieskomplikowanych zadań\cite{state-of-art}.
Nie wymaga specjalistycznych umiejętności programistycznych, co czyni ją dostępną dla szerszej grupy użytkowników.

\subsubsection{Żądania HTTP i parsowanie HTML}

Kolejną metodą scrapowania jest wykonywanie żądań HTTP w celu uzyskania struktury HTML danej strony internetowej.
Po pobraniu struktura HTML jest poddawana analizie w celu identyfikacji oraz ekstrakcji żądanych elementów, takich jak tekst, obrazy, hiperłącza i inne.
Często wykorzystuje się specjalistyczne biblioteki programistyczne, które umożliwiają wydobywanie poszczególnych elementów, m.in. poprzez zastosowanie wyrażeń XPath.
Metoda ta jest niezbędna w przypadku stron renderowanych statycznie, kiedy to dostęp do uporządkowanych danych z serwera API jest ograniczony lub niemożliwy.

\subsubsection{Żądania HTTP do serwerów API}

Obecnie, w dobie ogromnej popularności frameworków takich jak Angular oraz bibliotek jak React, duża część aplikacji internetowych jest renderowana dynamicznie.
Aplikacje te komunikują się z zapleczem technicznym (backendem) poprzez API (ang. \emph{Application Programming Interface}) wymieniając dane w uporządkowanej formie np. JSON lub XML.
Fakt ten wykorzystuje kolejna metoda scrapowania polegająca na wykonywaniu żądań bezpośrednio do tych interfejsów z ominięciem warstwy wizualnej aplikacji.
Jest to preferowana metoda, ponieważ znacząco redukuje trudności związane z ekstrakcją danych.
Proces ten jest zwykle szybszy i mniej podatny na błędy, które mogą wynikać ze zmian w strukturze HTML@\.
Należy zauważyć, że struktura HTML ulega częstszym zmianom niż kontakt API, co dodatkowo podkreśla efektywność tej metody w stabilnym pozyskiwaniu danych.

\subsubsection{Wykorzystanie przeglądarki internetowej}\label{subsubsec:browser-scraping-theory}

Wśród metod web scrapingu wyróżnia się także tę z wykorzystaniem przeglądarki internetowej.
Podstawę jej działania stanowią biblioteki programistyczne udostępniające wysokopoziomowe API przeglądarek internetowych.
W środowisku JavaScript za przykład posłużyć mogą Puppeteer bazujący na Chrome, Chromium i protokole DevTools
oraz Playwright wspierający silniki Chromium, WebKib i Firefox.
Pierwotnym zastosowaniem wspomnianych wyżej narzędzi było testowanie end-to-end, jednak z czasem rozszerzono ich wykorzystanie i funkcje wspierające web scraping.

Metoda ta sprawdzi się szczególnie w scrapowaniu aplikacji internetowych intensywnie wykorzystujących renderowanie JavaScript
lub kiedy wymagane jest środowisko przeglądarki np.~w przypadku zabezpieczenia przez Browser Fingerprinting~\cite{apify-headless-browsers}.
Opisywane biblioteki dysponują rozbudowanym wachlarzem funkcji dających pełną kontrolę nad przeglądarką.
Posiadają klasy takie jak strona, myszka, klawiatura i ekran dotykowy z zachowaniami umożliwiającymi nawigowanie po stronach,
ruchy kursorem i pisanie na klawiaturze.

Ponadto specjaliści w dziedzinie web scrapingu tworzą i rozwijają otwarto-źródłowe rozszerzenia wychodzące na przeciw zabezpieczeniom przeciwko scraperom.
Ukrywają ślad (ang. \emph{fingerprint}) wykorzystanych przeglądarek oraz maksymalnie upodobniają ich zachowanie do człowieka e.g.~przez powolniejsze i płynniejsze ruchy kursorem.

To wszystko sprawia, że takie scrapery są relatywnie łatwe do stworzenia.
W związku z wykorzystaniem przez nie rozbudowanego kontekstu przeglądarki wymagają większej ilości zasobów i w efekcie są mniej efektywne.

\subsection{Zastosowania Web Scrapingu}\label{subsec:web-scraping-applications}

Web scraping to jedno z najcenniejszych narzędzi w obszarze pracy z danymi.
Dzięki niemu możliwe stało się pozyskanie ogromnych ilości danych z niemal nieograniczonych zasobów internetu~\cite{Zhao2017}.
Dzięki automatyzacji i łatwemu dostępowi do ogromnych ilości danych, metoda ta zapewnia efektywne zbieranie informacji przy relatywnie niskich kosztach.
Przykładowymi obszarami, w których stosuje się web scraping są: Business Intelligence, Marketing i PR oraz Machine Learning i Artificial Intelligence.

\subsubsection{Business Intelligence}
Web Scraping może służyć za narzędzie, które pomaga firmom w podejmowaniu świadomych decyzji biznesowych i budowaniu przewagi konkurencyjnej.
Najczęściej zbieranymi informacjami są te dotyczące asortymentu konkurencji, cenach, promocjach, dostępności produktów, opinie klientów oraz dane kontaktowe.
Na rynku istnieją firmy, takie jak Doubledata~\cite{doubledata}, które sprzedają web scraping konkurencji w formie usługi.
Podczas przeglądania internetu i literatury, można skonstatować, że rynek e-commerce jest jednym z najczęstszych kontekstów, w którym omawiane jest zastosowanie web scrapingu.

\subsubsection{Marketing i PR}

Web Scraping odgrywa nieodzowną rolę w monitorowaniu treści internetowych i mediów społecznościowych, wśród których są Twitter, Facebook, Instagram, LinkedIn czy YouTube.
Stosowany jest do śledzenia opinii publicznej, obserwowania trendów oraz monitorowania nowo pojawiających się wzmianek o marce.
Automatyzacja tego procesu daje sposobność do szybkiej reakcji, co jest niezwykle ważne w efektywnym zarządzaniu reputacją marki~\cite{monitoring-social-media}.

\subsubsection{Machine Learning i Artificial Intelligence}

Naukowcy i inżynierowie wykorzystują web scraping do pozyskiwania danych niezbędnych do trenowania i modeli sztucznej inteligencji.
Dane te wykorzystywane są w różnych celach, poczynając od automatycznego rozpoznawania obrazu, na analizie języka naturalnego kończąc~\cite{openai-data-collection}.
